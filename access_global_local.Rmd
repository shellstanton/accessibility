---
title: "Accessibility - global vs local: MRes Project"
author: "John Archer"
date: "Jan - March 2021"
output: 
  html_document:
    keep_md: true
---


```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```


### Overview

This document aims to allow for fast and reproducible construction of friction surfaces detailing travel time to health facilities.
There are several key components which are necessary to construct a friction surface, with the aim of the surface detailing the cost to travel through gridded cells (pixels) of a known size. 
The work outlined here is produced at a 30m x 30m spatial resolution, with the area/country of interest being defined by the user.

The key steps of the below are:
1. Downloading Normalised Difference Vegetation Index (NDVI) data for the area of interest using Google Earth Engine.
  * The NDVI data will be used to determine cells for which the associated cost determines off-road travel. The premise for this is that dense vegetation (i.e. forests) will be more difficult to traverse than sparser vegetation (i.e. grass). Within this work, the biggest assumption is that **off-road travel will be achieved by foot/with a maximum speed** which is significantly slower than on-road travel.
2. Downloading open-source road network data for the area of interest using www.openstreetmap.org.
  * The network data will be used to determine cells for which the associated cost determines on-road travel. Roads available on OSM are categorised by their importance and usage and will be assigned different travel speeds (for example primary, secondary and tertiary roads).
3. Assigning travel speeds to cells associated with NDVI data, and roads. This will include trawling/gathering information for speeds associated with certain roads (using GPS data? national speed limits?) and walking through different densities of vegetation. 
4. Combining on-road and off-road data to produce one combined friction surface.
5. Identifying the location of health facilities to use within a cost-distance analysis using the friction surface.



### 1. Setup

1a. Setup: Downloading, installing and loading all required packages (from both CRAN and GitHub servers) 
CRAN packages.
```{r setup_CRAN_packages}

## Load required packages (available from CRAN) and install packages which are potentially missing on client computers
# Create vector list of CRAN packages needed
list.of.packages <- c("sf", 
                      "mapview",  
                      "googledrive",  
                      "osmdata", 
                      "ggplot2",  
                      "raster",  
                      "gdistance",  
                      "fasterize",  
                      "remotes",  
                      "rgdal", 
                      "stars", 
                      "geojsonio", 
                      "devtools", 
                      "rgee",
                      "tidyr",
                      "knitr")

# Identify which CRAN packages aren't currently installed on client computers and store in object
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]

# Install any missing CRAN packages
if(length(new.packages)) install.packages(new.packages)

# Load all required CRAN packages
lapply(list.of.packages, library, character.only = TRUE)

```

GitHub packages.
```{r setup_GitHub_packages}

## Download, install and load the following packages:
# * 'devtools': needed to download  packages from GitHub
# * 'wpgpDownloadR' and 'wpgpCovariates': needed to download raster datasets from the WorldPop FTP
# * 'afrimapr/afrihealthsites' package: needed to download healthcare facility data from a World Health Organisation (WHO) database via the afrimapr project

## Check if package is installed, if not, install
# 'devtools' package
if(!("devtools" %in% installed.packages())){

 install.packages("devtools")

}

# 'wpgpDownloadR' package
if(!("wpgpDownloadR" %in% installed.packages())){

 devtools::install_github("wpgp/wpgpDownloadR")

}

# 'wpgpCovariates' package
if(!("wpgpCovariates" %in% installed.packages())){

 devtools::install_github("wpgp/wpgpCovariates")
  
}

# 'afriMapR'afrimapr/afrihealthsites' package
if(!("afrimapr/afrihealthsites" %in% installed.packages())){

# 'afrimapr/afrihealthsites' package
 remotes::install_github("afrimapr/afrihealthsites")
  
}

## load packages
library(devtools)
library(wpgpDownloadR)
library(wpgpCovariates)
library(afrihealthsites)

```
All required packages should now be loaded.


1b. Setup: Define country of interest (coi), used to obtain:
* WHO health facility data (via afrimapr project)
* Population data (via WorldPop FTP)
```{r setup_country_of_intersest}

# * Set country of interest (coi) using British English spelling and capitalised first letter
coi <- "Malawi"

```


1c. Setup: Define area of interest (aoi)
* Detail limits for a bounding box (latitude/longitude maximum and minimum values using WGS84 projection) to be used as area of interest (aoi)
OR
* Read in shapefile to define area of interest
```{r setup_area_of_interest}

# * Detail limits of bounding box (latitude/longitude maximum and minimum values, in WGS84) to be used as area of interest (aoi)
bbxmin <- 33.2
bbxmax <- 33.8
bbymin <- -11.29
bbymax <- -10.73

# OR

# * Read in shapefile to define area of interest
# aoi <- readOGR(dsn = "USER_DIRECTORY", layer = "USER_SHAPEFILE")

```


1d. Setup: Landsat-8 parameters
* Read in Landsat-8 data, obtained using Google Earth Engine (GEE)
* Define start and end dates to filter Landsat-8 data by
This data will be used to calculate the median NDVI per pixel. NDVI values will then be used to generate friction surface and calculate off-road (on-foot) travel times.
```{r setup_landsat8_parameters}

# * Read in landsat-8 data
ls8_data <- "LANDSAT/LC08/C01/T1_RT_TOA"

# * Define start and end dates to filter data by
start_date <-  "2018-06-01"
end_date   <-  "2018-09-30"

```


1e. Setup: Define off-road (on-foot) travel speeds
* Define off-road (on-foot) travel speeds (non-vehicle, km/ hour) expected when traversing non-road pixels (defendant on NDVI value and customisable according to expected changes in travel speed, e.g., during the wet season).
```{r setup_walking_travel_speeds}

## Define walking speeds (in Km/h) expected for the following NDVI values:

# NDVI value = < 0.35 (impassable)
walk_speed_1 <- 0.1

# NDVI value = 0.35 - 0.6
walk_speed_2 <- 3.5

# NDVI value = 0.6 - 0.7
walk_speed_3 <- 2.48

# NDVI value = > 0.7
walk_speed_4 <- 1.49

```
Off-road (on-foot) travel speeds can be changed according to expected changes in travel speeds, e.g., during the wet season, etc.


1f. Setup: Define road travel speeds (km/ hour by motorcycle) for major and minor road types
* Define road travel speeds (km/ hour by motor vehicle) for major and minor road types expected when traversing road pixels. Road data is obtained using OpenStreetMaps and road travel speeds are customisable according to expected changes in travel speed, e.g., during the wet season.
```{r setup_walking_travel_speeds}

## Define road travel speeds (in Km/h) for the following road types:

# Major roads: on which national speed limits can typically be reached (e.g., motorway and trunk roads)
major_road_speed <- 80

# Minor roads: on which slower speeds would be expected (e.g., urban roads, dirt roads)
minor_road_speed <- 20

```
Road travel speeds can be changed according to expected changes in travel speeds, e.g., during the wet season, etc.


1g. Setup: Population parameters
* Obtain available covariate data from the WorldPop FTP (for country of interest)
* Select and set covariate of interest by viewing dataframe of available covariates downloaded from WorldPop FTP, which is later used to download population data from the WorldPop FTP
```{r setup_population_parameters}

# Run 'population_data' function, constructed to return dataframe of available covariates downloaded from WorldPop FTP (for country of interest) based on 'coi' (defined during setup stage 2: country of interest)
population_data <- 
  
  function(coi){
    
    # Obtain ISO3 country codes using 'wpgpListCountries' function
    ISO3_df <- wpgpListCountries()
  
    # Identify ISO3 country code for country of interest (coi) and store as object
    ISO3 <- ISO3_df[ISO3_df$Country == coi, "ISO3"]
  
    # Download dataset of available covariates for country of interest and store as object
    covariates_df <- wpgpListCountryDatasets(ISO3 = ISO3)
    
    # Return dataframe
    return(covariates_df)
    
}

# Select and set covariate of interest by viewing dataframe of available covariates downloaded from WorldPop FTP (for country of interest)
# View(population_data(coi))

# "ppp_2020": Estimated total number of people per grid-cell for year 2020

# Store chosen covaraite as object
covariate <- "ppp_2020"

```


1h. Setup: Connect to Google Earth Engine (GEE) via the 'rgee' package
'ee_install' function only needs to be run once.
```{r, eval=F}

## Only need to run once
# ee_install()

```
Once completed it should say 'Well done! rgee was successfully set up in your system.' and will then prompt you to restart your system. It also suggests running ee_check however there may currently be an issue with this function so I suggest you don't run it.

Then, initialise GEE. This will check whether you have everything set up to use GEE via R. If you don't, additional steps will be described.

Note that you need to link to a Google account that has been given GEE access to be able to complete this stage. As we'll also be using Google Drive for downloading and uploading data, we also need to include 'drive=TRUE'. 

Run 'ee_Initialize' function and follow all steps within the console.
```{r, eval=F}

ee_Initialize(drive = TRUE)

```
Now we can use R to run code on Google Earth Engine. Note that lots of examples of translating GEE syntax to be used in rgee can be found here: url{https://csaybar.github.io/rgee-examples/}.


*All setup stages are now complete*


2. Create polygon using xy combinations for each corner of the area of interest.
```{r, eval=F}

aoi <- ee$Geometry$Polygon(coords=list(c(bbxmin, bbymax), 
                                       c(bbxmax, bbymax),
                                       c(bbxmax, bbymin), 
                                       c(bbxmin, bbymin)))

```



### Landsat-8 satellite data
3. Read in Landsat-8 Tier 1 satellite data
```{r, eval=F}

ls8 <- ee$ImageCollection(ls8_data)

```

Filter the LS8 collection by area of interest and then by collection date
```{r, eval=F}

# Filter by area of interest
spatialFiltered <- ls8$filterBounds(aoi)

# Filter by collection date
temporalFiltered <- spatialFiltered$filterDate(start_date, end_date)

```

4. Create a cloud mask, apply the mask and calculate NDVI for the unmasked pixels
```{r, eval=F}

ndvilowcloud <- function(image) {
  # Get a cloud score in [0, 100].
  cloud <- ee$Algorithms$Landsat$simpleCloudScore(image)$select('cloud')

  # Create a mask of cloudy pixels from an arbitrary threshold (20%).
  mask <- cloud$lte(20)

  # Compute NDVI using inbuilt function
  ndvi <- image$normalizedDifference(c('B5', 'B4'))$rename('NDVI')

  # Return the masked image with an NDVI band.
  image$addBands(ndvi)$updateMask(mask)
}


cloudlessNDVI = temporalFiltered$map(ndvilowcloud)

```

5. Calculate the median NDVI per pixel and clip to the area of interest
```{r, eval=F}

medianimage <- cloudlessNDVI$median()$select('NDVI')

medNDVIaoi <- medianimage$clip(aoi)

```

View output
```{r, eval=F}

Map$centerObject(aoi)

Map$addLayer(
  eeObject=medNDVIaoi,
  visParam=list(min=-1, max=1, palette=c('blue', 'white', 'green')),
  name="Median NDVI"
)

```


6. Convert image to raster and download it using Google Drive (drive) or Google Cloud Storage (GCS)
* These data are saved as an image within google earth engine (GEE). 
* Convert data to raster and download using drive or GCS.
* Raster is stored as .tif file in a temporary local folder, which can then be written to our data folder. 
* More information on this process can be found here: url{https://r-spatial.github.io/rgee/reference/ee_as_raster.html} 
```{r, eval=F}

med_ndvi <- ee_as_raster(
  image = medNDVIaoi,
  region = aoi,
  scale = 30,
  via = 'drive'
)

```

The TIFF file is stored in a temporary folder, which we can then write to our data folder. I'll write the code to save this as NDVIgee.tif for now rather than overwrite the NDVIexample.tif that is currently there (although both should be the same). The 'eval' parameter is still set to FALSE.
```{r, eval=F}

writeRaster(med_ndvi,
            "./data/NDVIgee",
            format = "GTiff", 
            overwrite=TRUE)

```
*https://rpubs.com/ials2un/getlandsat*
*Worth looking into this - implies we can source NDVI from Landsat 8 directly within R. It would require users to enter their USGS log in details but something to investigate, and associated details can be an input of the Shiny app.*



### OpenStreetMap data
7. To detail travel speeds within our area of interest, we are utilising an open source road network which is publicly compiled and hosted on:
www.openstreetmap.org. 

We can directly download OSM road data for our aoi within R. The bounding box is in the format c(xmin, ymin, xmax, ymax), using the limits defined earlier in the script.
```{r osmdownload}

## define bounding box
aoi_bbox = c(bbxmin, 
             bbymin, 
             bbxmax, 
             bbymax)

## obtain road data
q <- opq(bbox = aoi_bbox) %>%
     add_osm_feature(key = 'highway') %>%
     osmdata_sf()


# Plot road data to check
ggplot(q$osm_lines) + 
  geom_sf()

```



### Assign travel speeds
8a: Assign off-road (on-foot) travel speeds, dependent on NDVI values
*Need to document how these speeds are determined and how they vary by season and location for example*.
```{r ndvireclass}

# Temporarily read in NDVIexample from folder, which has been directly downloaded from GEE.
# We could replace this with med_ndvi if rgee continues to be reliable.
ndvipath <- "./data/NDVIexample.tif"

ndvi <- raster(ndvipath)

# Reclassify according to walking speeds, defendant on NDVI value, defined during setup stage 4
ndviwalk_kph <- c(walk_speed_1,
                  walk_speed_2,  
                  walk_speed_3,
                  walk_speed_4)

# Convert to m/s
ndviwalk_mps <- ndviwalk_kph/3.6

# Convert to crossing time in seconds, assuming travel along hypotenuse and pixel size is 30m
ndviwalk_secs <- 42.43/ndviwalk_mps

# Convert km/h to m/s
ndviwalk_vec <- c(-1, 0.35, ndviwalk_secs[1], 
                  0.35, 0.6, ndviwalk_secs[2], 
                  0.6, 0.7, ndviwalk_secs[3], 
                  0.7, 1, ndviwalk_secs[4])

ndviwalk_mat <- matrix(ndviwalk_vec, ncol = 3, byrow = TRUE)

ndvi_assigned <- ndvi

ndvi_assigned <- reclassify(ndvi_assigned, ndviwalk_mat)

```

8b: Assign on-road (by motor vehicle) travel speeds to OpenStreetMaps (OSM) road data
*Again, need to document how these speeds are determined and how they may vary by season*.
* We are currently using national speed limits as a maximum for primary and major roads.
```{r toraster, warning=FALSE}

# Primary = 80kph, secondary = 80kph, 'Other' road speed = 20 kph
road_vector <- c("primary", "secondary", "motorway", "trunk")

q$osm_lines$motorspeedkph <- ifelse(q$osm_lines$highway %in% road_vector, major_road_speed, minor_road_speed)

q$osm_lines$motorspeedmps <- q$osm_lines$motorspeedkph/3.6

# Assume a 30m resolution cell
q$osm_lines$time_secs <- 42.43/q$osm_lines$motorspeedmps

# Convert to raster, matching up with the NDVI raster resolution and extent
# Note that the fasterize function only works with polygons, so adding a buffer to the roads of ~30m
roads.poly <- st_buffer(q$osm_line, 0.00015)

osm_road_raster <- fasterize(roads.poly, ndvi_assigned, "time_secs", fun = 'min')

```


9. Merge ‘NVDI’ raster (ndvi_assigned) and road-data raster (osm_road_raster)
* The next step is to merge data for on-road and off-road travel to create one cohesive friction surface.
* In areas where "road" and "off-road" cells overlap, the road values will be retained as these will be associated with the lowest cost (quickest speed).
```{r genfriction}

## merge the NDVI and the OSM, retain the minimum value (this is the quickest cell crossing time)
friction_surface_motor <- mosaic(osm_road_raster, ndvi_assigned, fun = min, tolerance = 1)


writeRaster(friction_surface_motor,
            "./outputs/friction_raster_motor",
            format = "GTiff", overwrite=TRUE)

```



### Heatlh facility data
10. Download and prepare health facility location data
* Download prepare and view health facility location data from a World Health Organisation (WHO) database
* WHO health facility location data is obtained using the afrihealthsites function, as part of the afrimapr/afrihealthsites package.
```{r}

## WHO data
mwi_healthfac_who <-  afrihealthsites(coi,
                                      datasource = 'who')

# Check
# mwi_healthfac_who_df <- as.data.frame(mwi_healthfac_who)
# nrow(mwi_healthfac_who_df)


# Covert to SpatialPointsDataFrame class
mwi_healthfac_who_spdf <- as(mwi_healthfac_who, "Spatial")
# SpatialPointsDataFrame 

# Crop to extent of 'friction_surface_motor' raster
mwi_healthfac_who_spdf_cropped <- raster::crop(mwi_healthfac_who_spdf, y = extent(friction_surface_motor))

# To view health facility location data, save mwi_healthfac_who_spdf_cropped as dataframe
mwi_healthfac_who_data <- as.data.frame(mwi_healthfac_who_spdf_cropped)
# Check
# View(mwi_healthfac_who_data)
# kable(mwi_healthfac_who_data)

```



### Calculate shortest paths
11. Calculate shortest paths (quickest time) from the closest-proximity health facility
```{r, warning=FALSE}

# First calculate the transition matrix
# Check the transitionFunction
trans_motor <- transition(friction_surface_motor, transitionFunction = function(x){1/mean(x)}, directions = 8)

# Then calculate the cumulative cost
leastcost_motor <-  accCost(trans_motor, mwi_healthfac_who_spdf_cropped)

writeRaster(leastcost_motor,
            "./outputs/leastcost_raster_motor",
            format = "GTiff", 
            overwrite=TRUE)

```



### Create plot
12. Create plot to visualise 'leastcost_motor' raster data (cost-distance analysis from closest-proximity health facility)
```{r, warning=FALSE}

## warning for removing NA pixels is masked
lcm_df <- as.data.frame(leastcost_motor, xy = TRUE)

lcm_df$mins <- lcm_df$layer/60

# Create simple features POINT geometry from health facility data to plot
healthfac_data_plot <- st_as_sf(mwi_healthfac_who_spdf_cropped)

## Plot
# Time-boundary thresholds from closest-proximity health facility set to:
# < 30 minutes
# < 1 hour
# < 3 hours
# < 6 hours
# < 12 hours
# < 24 hours
#> 24 hours

ggplot()+
    geom_raster(data=lcm_df, aes(x = x, y = y, fill = cut(mins, c(0,30,60,120,180,360,720,max(mins)))))+
    scale_fill_brewer(palette = "YlGnBu")+
    geom_sf(data=q$osm_lines, colour="darkgrey", alpha=0.3)+
    geom_sf(data=healthfac_data_plot, size=2, colour="red")+
    guides(fill=guide_legend(title="Time (mins)"))

```



### Population data
13. Download population data (.tif) for country and covariate of interest (Malawi/entire population in 2020) from the WorldPop FTP and create raster from downloaded data
```{r download_population_data_and_create_raster}

# Download  and read in dataset (.tif) for country and covariate of interest (based on ISO3 and covariate name set during setup stages)
pop_data <- 
  
  wpgpGetCountryDataset(ISO3 = ISO3,
                        covariate = covariate, 
                        destDir = "./WorldPop")
# Note: Downloaded .tif is ~50 MB: This is the entire country of Malawi

# Create raster
pop_data <- raster(pop_data)

# Check
pop_data
# CRS assigned: WGS84

```


14. Resample 'pop_data' raster to match resolution of 'leastcost_motor'.
```{r resample_pop_data}

# Determine 'pop_data' resolution 
res(pop_data)
# 0.0008333333 0.0008333333

# Determine 'leastcost_motor' resolution
leastcost_motor
res(leastcost_motor)
# 0.0002694946 0.0002694946

## Use 'resample' function to:
# * Resample 'pop_data' to match resolution of 'leastcost_motor'
# * Clip 'pop_data' to extent of 'leastcost_motor'
pop_data <- resample(pop_data, leastcost_motor, method = "bilinear")

# Check
res(pop_data) == res(leastcost_motor)
# True
extent(pop_data) == extent(leastcost_motor)
# True

# Check CRS
crs(pop_data)
# WGS84

# Visualise raster
# plot(pop_data)

```


15. Reclassify 'leastcost_motor' raster to chosen time-boundary categorical zones.
```{r reclassify}

## Create new raster from 'lcm_df' dataframe
# 'rasterfromXYZ' function works only with three columns, so remove 'layer' column from 'lcm_df'
lcm_df$layer = NULL
# Check
# View(lcm_df)

# Create new raster from 'lcm_df' dataframe
lcm_raster <- rasterFromXYZ(lcm_df)
# Check
# plot(lcm_raster)

## Create matrix of time-boundary categories of interest (to resample 'lcm_raster')
## Categories: 
# * < 30 minutes
# * 30 minutes - 1 hour
# * 1 hour - 3 hours
# * 3 hours - 6 hours
# * 6 hours - 12 hours
# * 12 hours - 24 hours
rcl_matrix <- c(0, 30, 1, # 30 minutes
                30, 60, 2, # 1 hour
                60, 180, 3, # 3 hours
                180, 360, 4, # 6 hours 
                360, 720, 5, # 12 hours
                720, max(lcm_df$mins), 6 # 12+ hours
)

rcl_matrix <- matrix(rcl_matrix, ncol=3, byrow=TRUE)
# Check
# View(rcl_matrix)

## Reclassify 'lcm_raster' using 'rcl_matrix'  according to time-boundary categorical zones
lcm_pop_data_rcl <- reclassify(lcm_raster, rcl_matrix, include.lowest=TRUE)
# Check
lcm_pop_data_rcl
# Rasterlayer
# CRS not assigned

## Assign CRS to that of 'leastcost_motor' (WGS84)
projection(lcm_pop_data_rcl) <- crs(leastcost_motor)
# Check
crs(lcm_pop_data_rcl)
# CRS set (WGS84)

# Check
# plot(lcm_pop_data_rcl)

# Check as data.frame
lcm_rcl_pop_data_df <- as.data.frame(lcm_pop_data_rcl, xy = TRUE)
# View(lcm_rcl_pop_data_df)

```


16. Determine population within each time-boundary zone. Summate the data to give total number of individuals within chosen time-boundary categorical zones and calculate % of total population within chosen time-boundary categorical zones.
```{r determine_population_within_time-boundary_zones}

# Determine population (within 'pop_data' raster), within time-boundary  zones (within 'lcm_pop_data_rcl') using 'zonal' function 
lcm_rcl_zone <- zonal(pop_data, lcm_pop_data_rcl, fun = sum)

# Create dataframe
lcm_rcl_zone_df <- as.data.frame(lcm_rcl_zone, xy = TRUE)

# Check
# View(lcm_rcl_zone_df)

# Rename columns 
colnames(lcm_rcl_zone_df)
names(lcm_rcl_zone_df)[1] <- "Zone"
names(lcm_rcl_zone_df)[2] <- "Zone Population"
# View(lcm_rcl_zone_df)

# Replace time-boundary zone codes with chosen time-boundary categories
lcm_rcl_zone_df$Zone <- c("< 30 minutes",
                          "30 minutes - 1 hour",
                          "1 hour - 3 hours",
                          "3 hours - 6 hours",
                          "6 hours - 12 hours",
                          "12 hours - 24 hours")

# Add 'Total Population'  column
lcm_rcl_zone_df$" Total Population" <-
  
  c(sum(lcm_rcl_zone_df$"Zone Population"[1]),
    sum(lcm_rcl_zone_df$"Zone Population"[1:2]),
    sum(lcm_rcl_zone_df$"Zone Population"[1:3]),
    sum(lcm_rcl_zone_df$"Zone Population"[1:4]),
    sum(lcm_rcl_zone_df$"Zone Population"[1:5]),
    sum(lcm_rcl_zone_df$"Zone Population"[1:6]))

# Add % of total population column
lcm_rcl_zone_df$"% Population" <-
   
  c(sum(lcm_rcl_zone_df$"Zone Population"[1])   / sum(lcm_rcl_zone_df$"Zone Population"[1:6]) * 100,
    sum(lcm_rcl_zone_df$"Zone Population"[1:2]) / sum(lcm_rcl_zone_df$"Zone Population"[1:6]) * 100,
    sum(lcm_rcl_zone_df$"Zone Population"[1:3]) / sum(lcm_rcl_zone_df$"Zone Population"[1:6]) * 100,
    sum(lcm_rcl_zone_df$"Zone Population"[1:4]) / sum(lcm_rcl_zone_df$"Zone Population"[1:6]) * 100,
    sum(lcm_rcl_zone_df$"Zone Population"[1:5]) / sum(lcm_rcl_zone_df$"Zone Population"[1:6]) * 100,
    sum(lcm_rcl_zone_df$"Zone Population"[1:6]) / sum(lcm_rcl_zone_df$"Zone Population"[1:6]) * 100)
  

# Check
lcm_rcl_zone_df

# Create new dataframe detailing number and percent (%) of population residing within pre-defined time-boundaries from the closest proximity health centre
time_boundaries <- c("< 30 minutes", "< 1 hour", "< 3 hours", "< 6 hours", "< 12 hours", "< 24 hours")

# Create dataframe
FS_output <- data.frame(time_boundaries,
                        lcm_rcl_zone_df$` Total Population`,
                        lcm_rcl_zone_df$`% Population`)

# Rename columns
names(FS_output)[1] <- "Time boundaries"
names(FS_output)[2] <- "Number population"
names(FS_output)[3] <- "Percent (%) population"

# Check output using Kable formatting
FS_output_kable <- kable(FS_output,
                   caption = "Number and percent (%) of population residing within pre-defined time-boundaries from the closest proximity health centre")

# Check
FS_output_kable

```


17. Create blot bar plot to visualise % of total population within chosen time-boundary categorical zones
```{r}

# Create new dataframe
bar_plot_df <- data.frame(FS_output$`Time boundaries`,
                          FS_output$`Percent (%) population`)

# Check  
# bar_plot_df  

# Rename columns 
colnames(bar_plot_df)
names(bar_plot_df)[1] <- "Time-boundary"
names(bar_plot_df)[2] <- "% Population"
colnames(bar_plot_df)

# Check  
# bar_plot_df  

# Create bar plot
FS_barplot <- 
  
  ggplot(data = bar_plot_df, aes(x = bar_plot_df$`Time-boundary`, y = bar_plot_df$`% Population`)) +
  geom_bar(aes(fill = "% Population"), width = 0.4, position = position_dodge(width=0.5), stat="identity") +
  scale_x_discrete(limits = bar_plot_df$`Time-boundary`) +
  theme_light() +
  ylab("Percent (%) of population") +
  xlab("Time-boundary category") +
  ggtitle("Percent (%) of population residing within pre-defined time-boundaries from the closest 
proximity health centre") +
  theme(plot.title = element_text(size = 9),
        legend.title = element_blank(),
        axis.title=element_text(size = 9),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


# Check
FS_barplot

```


