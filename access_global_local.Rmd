---
title: "Accessibility - global vs local"
author: "John Archer"
date: "Jan - March 2021"
output: 
  html_document:
    keep_md: true
---


### MRes mini project: Task_8

Restructure code so it contains an initial section for user inputs relating to:
* area of interest
* road travel speeds
* NDVI parameters i.e. period of time over which to summarise NDVI plus walking speeds assigned to NDVI intervals
JL:
* I envisage that most users won't know the coordinates for the bounding box of their area of interest, but may have a shapefile, or know the name of a district.
* Would it be useful to allow the option of a data-upload at the start of the script? We can grab the extent from a spatial object. 

```{r setup_JRA, include=FALSE}

# Forumate setup chunk

```





```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```


```{r packages, warning=FALSE, message=FALSE, results=FALSE}

## load required packages, and install packages which are potentially missing on client computers
list.of.packages <- c("sf", "mapview", "googledrive", "osmdata", "ggplot2", "raster", "gdistance", "fasterize", "remotes", "rgdal", "stars", "geojsonio")

new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]

if(length(new.packages)) install.packages(new.packages)

lapply(list.of.packages, library, character.only = TRUE)

```


### Overview

This document aims to allow for fast and reproducible construction of friction surfaces detailing travel time to health facilities.
There are several key components which are necessary to construct a friction surface, with the aim of the surface detailing the cost to travel through gridded cells (pixels) of a known size. 
The work outlined here is produced at a 30m x 30m spatial resolution, with the area/country of interest being defined by the user.
The key steps of the below are:
1. Downloading Normalised Difference Vegetation Index (NDVI) data for the area of interest using Google Earth Engine.
  * The NDVI data will be used to determine cells for which the associated cost determines off-road travel. The premise for this is that dense vegetation (i.e. forests) will be more difficult to traverse than sparser vegetation (i.e. grass). Within this work, the biggest assumption is that **off-road travel will be achieved by foot/with a maximum speed** which is significantly slower than on-road travel.
2. Downloading open-source road network data for the area of interest using www.openstreetmap.org.
  * The network data will be used to determine cells for which the associated cost determines on-road travel. Roads available on OSM are categorised by their importance and usage and will be assigned different travel speeds (for example primary, secondary and tertiary roads).
3. Assigning travel speeds to cells associated with NDVI data, and roads. This will include trawling/gathering information for speeds associated with certain roads (using GPS data? national speed limits?) and walking through different densities of vegetation. 
4. Combining on-road and off-road data to produce one combined friction surface.
5. Identifying the location of health facilities to use within a cost-distance analysis using the friction surface.

### NDVI data generation using rgee
I keep having issues with the rgee package, so I suggest at some point we split this Rmd file so that we keep the GEE part in one file, and the cost-distance surface generation in another. Users can then either have the option of generating the NDVI in R, or using GEE directly, with the link to the code provided.

```{r}

## install rgee from github
# remotes::install_github("r-spatial/rgee")

library(rgee)

```


This first bit is all about connecting to GEE via the rgee package. You should only need to run this next part once, but I keep having issues and have to start from scratch. 

```{r, eval=F}

# Only need to run once
# ee_install()

```


Once completed it should say 'Well done! rgee was successfully set up in your system.' and will then prompt you to restart your system. It also suggests running ee_check however there may currently be an issue with this function so I suggest you don't run it.
Then, initialise GEE. This will check whether you have everything set up to use GEE via R. If you don't additional steps will be described.

We then initialise the rgee package. Once rgee is installed I believe you only need to do this once.

Note that you need to link to a Google account that has been given GEE access to be able to complete this stage. As we'll also be using Google Drive for downloading and uploading data, we also need to include 'drive=TRUE'. 

```{r, eval=F}

ee_Initialize(drive = TRUE)

```


Now we can use R to run code on Google Earth Engine. Note that lots of examples of translating GEE syntax to be used in rgee can be found here url{https://csaybar.github.io/rgee-examples/}.

First, define the area of interest:

Start by detailing limits for a bounding box (lat/long maximum and minimum values, in WGS84).

```{r bblims}

## max and minimum values for longitude and latitude
bbxmin <- 33.2
bbxmax <- 33.8
bbymin <- -11.29
bbymax <- -10.73

```


Then make this into a polygon by using xy combinations for each corner of the area of interest.

```{r, eval=F}

aoi <- ee$Geometry$Polygon(coords=list(c(bbxmin, bbymax), c(bbxmax, bbymax), c(bbxmax, bbymin), c(bbxmin, bbymin)))

```


Read in the Landsat 8 Tier 1 dataset

```{r, eval=F}

ls8 <- ee$ImageCollection("LANDSAT/LC08/C01/T1_RT_TOA")

```


Filter the LS8 collection by area & then by collection date

```{r, eval=F}

spatialFiltered <- ls8$filterBounds(aoi)

temporalFiltered <- spatialFiltered$filterDate('2018-06-01', '2018-09-30')

```


Create a cloud mask, apply the mask and calculate NDVI for the unmasked pixels

```{r, eval=F}

ndvilowcloud <- function(image) {
  # Get a cloud score in [0, 100].
  cloud <- ee$Algorithms$Landsat$simpleCloudScore(image)$select('cloud')

  # Create a mask of cloudy pixels from an arbitrary threshold (20%).
  mask <- cloud$lte(20)

  # Compute NDVI using inbuilt function
  ndvi <- image$normalizedDifference(c('B5', 'B4'))$rename('NDVI')

  # Return the masked image with an NDVI band.
  image$addBands(ndvi)$updateMask(mask)
}


cloudlessNDVI = temporalFiltered$map(ndvilowcloud)

```


Calculate the median NDVI per pixel and clip to the area of interest

```{r, eval=F}

medianimage <- cloudlessNDVI$median()$select('NDVI')

medNDVIaoi <- medianimage$clip(aoi)

```


View output

```{r, eval=F}

Map$centerObject(aoi)

Map$addLayer(
  eeObject=medNDVIaoi,
  visParam=list(min=-1, max=1, palette=c('blue', 'white', 'green')),
  name="Median NDVI"
)

```


These data are all stored as an image within GEE. We can convert this image to a raster and download it using Google drive (drive) or Google Cloud Storage (gcs). More information on this can be obtained here url{https://r-spatial.github.io/rgee/reference/ee_as_raster.html} 

```{r, eval=F}

med_ndvi <- ee_as_raster(
  image = medNDVIaoi,
  region = aoi,
  scale = 30,
  via = 'drive'
)

```


The TIFF file is stored in a temporary folder, which we can then write to our data folder. I'll write the code to save this as NDVIgee.tif for now rather than overwrite the NDVIexample.tif that is currently there (although both should be the same). The 'eval' parameter is still set to FALSE.

```{r, eval=F}

writeRaster(med_ndvi,
            "./data/NDVIgee",
            format = "GTiff", 
            overwrite=TRUE)

```


### OpenStreetMap data
To detail travel speeds within our area of interest, we are utilising an open source road network which is publicly compiled and hosted on:
www.openstreetmap.org. 

We can directly download OSM road data for our aoi within R. The bounding box is in the format c(xmin, ymin, xmax, ymax), using the limits defined earlier in the script.

```{r osmdownload}

## define bounding box
aoi_bbox = c(bbxmin, bbymin, bbxmax, bbymax)

## obtain road data
q <- opq(bbox = aoi_bbox) %>%
    add_osm_feature(key = 'highway') %>%
    osmdata_sf()

ggplot(q$osm_lines)+geom_sf()

```


### Assign speeds
We now want to assign speeds to the NDVI pixels plus the roads

#### NDVI pixels by walking speed
Need to document how these speeds are determined and how they vary by season and location for example.

```{r ndvireclass}

# Temporarily read in NDVIexample from folder, which has been directly downloaded from GEE.
# We could replace this with med_ndvi if rgee continues to be reliable.
ndvipath <- "./data/NDVIexample.tif"

ndvi <- raster(ndvipath)

# Reclassify so that <0.35 = impassable, 0.35-0.6 = 3.5km/h, 0.6-0.7 = 2.48km/h and > 0.7 = 1.49km/h 
ndviwalk_kph <- c(0.1, 3.5,  2.48, 1.49)

# Convert to m/s
ndviwalk_mps <- ndviwalk_kph/3.6

# Convert to crossing time in seconds, assuming travel along hypotenuse and pixel size is 30m
ndviwalk_secs <- 42.43/ndviwalk_mps

  
# Convert km/h to m/s
ndviwalk_vec <- c(-1, 0.35, ndviwalk_secs[1], 0.35, 0.6, ndviwalk_secs[2], 0.6, 0.7, ndviwalk_secs[3], 0.7, 1, ndviwalk_secs[4])

ndviwalk_mat <- matrix(ndviwalk_vec, ncol = 3, byrow = TRUE)

ndvi_assigned <- ndvi

ndvi_assigned <- reclassify(ndvi_assigned, ndviwalk_mat)

```


#### Roads by motor vehicle
Again, need to document how these speeds are determined and how they may vary by season.
We are currently using national speed limits as a maximum for primary and major roads.

```{r toraster, warning=FALSE}

# Primary = 80kph, secondary = 80kph, 'Other' road speed = 20 kph
road_vector <- c("primary", "secondary", "motorway", "trunk")

q$osm_lines$motorspeedkph <- ifelse(q$osm_lines$highway %in% road_vector, 80, 20)

q$osm_lines$motorspeedmps <- q$osm_lines$motorspeedkph/3.6

# Assume a 30m resolution cell
q$osm_lines$time_secs <- 42.43/q$osm_lines$motorspeedmps

# Convert to raster, matching up with the NDVI raster resolution and extent
# Note that the fasterize function only works with polygons, so adding a buffer to the roads of ~30m
roads.poly <- st_buffer(q$osm_line, 0.00015)

osm_road_raster <- fasterize(roads.poly, ndvi_assigned, "time_secs", fun = 'min')

```


#### Merge NDVI and road rasters together
The next step is to merge data for on-road and off-road travel to create one cohesive friction surface.
In areas where "road" and "off-road" cells overlap, the road values will be retained as these will be associated with the lowest cost (quickest speed).

```{r genfriction}

## merge the NDVI and the OSM, retain the minimum value (this is the quickest cell crossing time)
friction_surface_motor <- mosaic(osm_road_raster, ndvi_assigned, fun = min, tolerance = 1)


writeRaster(friction_surface_motor,
            "./outputs/friction_raster_motor",
            format = "GTiff", overwrite=TRUE)
```


### Calculate shortest paths
First, add in health facility locations. Currently I'm reading in the facilities around Vwaza that have rHAT diagnostics, but will edit this to use afrimapr.

```{r, message=FALSE, results=FALSE}

healthfac <- st_read("./data/healthfacexample.shp")

```

```{r}

# First calculate the transition matrix
# Check the transitionFunction
trans_motor <- transition(friction_surface_motor, transitionFunction = function(x){1/mean(x)}, directions=8)

# Then calculate the cumulative cost
leastcost_motor <-  accCost(trans_motor, as_Spatial(healthfac))

writeRaster(leastcost_motor,
            "./outputs/leastcost_raster_motor",
            format = "GTiff", overwrite=TRUE)

```


Now create some plots of the data.

```{r, warning=FALSE}

## warning for removing NA pixels is masked
lcm_df <- as.data.frame(leastcost_motor, xy = TRUE)

lcm_df$mins <- lcm_df$layer/60

ggplot()+geom_raster(data=lcm_df, aes(x = x, y = y, fill = cut(mins, c(0,30,60,120,180,240,300,max(mins)))))+
    scale_fill_brewer(palette = "YlGnBu")+
    geom_sf(data=q$osm_lines, colour="darkgrey", alpha=0.3)+
    geom_sf(data=healthfac, size=2, colour="red")+
    guides(fill=guide_legend(title="Time (mins)"))

```





### MRes_mini_project: Task_7

Add in population rasters, and calculate the number/proportion of the population living within a range of travel time thresholds. There are R packages for Worldpop which may be of use (https://www.worldpop.org/sdi/plugins), plus Worldpop is in GEE so it's probably easiest to use this dataset. It's at 100m resolution.

*Landsat data is at 30m x 30m resolution, whereas some of the dataset's Michelle's linked to are at a larger spatial resolution. For some of the extraction functions, the rasters will need to be at the same spatial resolution. Some resampling would be useful here.
*You will need to check projections of the different spatial datasets to ensure they align (extract and crop functions won't work if the data is in different spatial resolutions).
*You'll have to determine some sensible thresholds to quantify the population by (i.e. within 30 minutes, 1hr, etc... travel of a facility).
*You may want to further categorise the population as "at-risk" or "not at-risk" of infection. I think we should get the ball rolling with just one population surface for the moment and this is something we can code in later down the line.


### Option_1: Extract time-boundary polygons from 'leastcost_motor' ???
```{r isolate_time_boundary_polygons, include=FALSE}

### Extract points (< 30 mins from health facilities) from leastcost_motor then create polygon ???

# WIP
# extract(leastcost_motor, XXX?,)

test <- rasterToPolygons(leastcost_motor, 
                         fun = function(x){x < 1000},
                         dissolve = TRUE)

test
plot(test)

```


### Option_2: Subset time-boundary data from data.frame, create polygon(s) and submit to WorldPop ???
```{r subset_dataframe_and_submit_polygon_to_WorldPop, include=FALSE}

# Load dplyr
library(dplyr)

# View dataset
View(lcm_df)

# Subset by < 30 mins 
thirty_min <- leastcost_motor_pop_df[leastcost_motor_pop_df$mins < 30, ]

# Check
View(thirty_min)

# Remove 'layer' collumn to give just three rows (needed for 'rasterfromXYZ' function)
thirty_min$layer <- NULL

# Check with plot
ggplot()+geom_raster(data=thirty_min, aes(x = x, y = y, fill = cut(mins, c(0,30,max(mins)))))+
    scale_fill_brewer(palette = "YlGnBu")+
    geom_sf(data=q$osm_lines, colour="darkgrey", alpha=0.3)+
    geom_sf(data=healthfac, size=2, colour="red")+
    guides(fill=guide_legend(title="Time (mins)"))

# Create raster
thirty_min_raster <- rasterFromXYZ(thirty_min)

# Check
# View(thirty_min_raster)
thirty_min_raster
# RasterLayer
# No CRS assigned...
plot(thirty_min_raster)

# Create polygon (takes about 5-10 mins)
thirty_min_polygon <- rasterToPolygons(thirty_min_raster, dissolve = TRUE)

# Check
thirty_min_polygon
# Is a SpatialPolygonsDataFrame (spdf)
# Has no CRS? Attempted to assign WGS84 but was then unable to use plot() function...
plot(thirty_min_polygon)
# A solid shape, whereas we need boundary data

# Aggregate 'thirty_min_polygon' to give boundary 
thirty_min_polygon_agg <- aggregate(thirty_min_polygon)

# Check
# plot(thirty_min_polygon_agg)

thirty_min_polygon_agg
# Class: SpatialPolygons 

# Save as shapefile
writeOGR(thirty_min_polygon_agg_df, dsn = "thirty_min_polygon_agg_df", layer = "thirty_min_polygon_agg_df", driver = "ESRI Shapefile", overwrite=TRUE)

st_write(thirty_min_polygon_agg, dsn = "thirty_min_polygon_agg", layer = "thirty_min_polygon_agg")

# Cannot save as shapefile as 'thirty_min_polygon_agg' is 'SpatialPolygons' and needs to be 'SpatialPolygonsDataFrame'
# Convert 'thirty_min_polygon_agg' to SpatialPolygonsDataFrame
thirty_min_polygon_agg_df <- SpatialPolygonsDataFrame(thirty_min_polygon_agg, data = as.data.frame("thirty_min_polygon_agg_df"))
## But this saves as SpatialPolygonsDataFrame? Not polygon?

## Install and use wpCPR : An R package to submit a custom polygon request to the WorldPop API
# install.packages("devtools")
#devtools::install_github("wpgp/wpCPR")

# load package
library(wpCPR)

wpCPRPopulation (year=2020,
                 shapeFilePath = "./thirty_min_polygon_agg_df/thirty_min_polygon_agg_df.shp",
                 outputFilePath = NULL,
                 apikey = NULL,
                 callbacktime = 5,
                 maxexectime = 3600,
                 apiurl = NULL,
                 verbose = TRUE)

## Error: Request failed :: Unsupported Geometry:  This operation supports only Polygons.

```
### Unable to save SpatialPolygonsDataFrame as Polygon shapefile and so cannot upload to World Pop...


### Option_3: Download population rasters from WorldPop, subset time-boundary data, create polygon(s) and extract from population rasters using polygon(s) (Working). 
```{r download_population_data_raster, include=FALSE}

### Add in raster population dataset layers ???

## Install and use wpgpCovariates: an R Package interface for downloading raster datasets from WorldPop FTP.
# install.packages("devtools")
# devtools::install_github("wpgp/wpgpDownloadR")

# load package
library(wpgpDownloadR)

## Download ISO3 dataframe to obtain ISO3 info
# 'wpgpListCountries' function will return a dataframe with all ISO3 available on WorldPop ftp server.
ISO3_df <- wpgpListCountries()
# View(ISO3_df)

# Set country of interest (move to beginning of script within initial setup phase?)
COI <- "Malawi"

# Ascertain ISO3 of Malawi
ISO3 <- ISO3_df[ISO3_df$Country == COI, "ISO3"]
ISO3

# 'wpgpListCountryDatasets' function will return a dataframe of available covariates to download from WorldPop FTP for a country. 
# This function could be used to query the name of the dataset which then could be downloaded for a country.
covariates <- wpgpListCountryDatasets(ISO3 = ISO3)
# Check data and check for covariates of interest
View(covariates)

# 'wpgpGetCountryDataset' function will download a raster dataset based on ISO and covariate name
wpgpGetCountryDataset(ISO3 = ISO3,
                      covariate = "ppp_2020",
                      destDir = "./WorldPop")


## .tif file is ~50 MB
# Read in .tif file
pop_data <- "./WorldPop/mwi_ppp_2020.tif"
pop_data <- raster(pop_data)

# Check
pop_data
# plot(pop_data)
res(pop_data)
# 0.0008333333 0.0008333333

# Check raster resampling to ('leastcost_motor')
leastcost_motor
res(leastcost_motor)
# 0.0002694946 0.0002694946

## Need to resample to match resolution
pop_data <- resample(pop_data, leastcost_motor, method = "bilinear")

# Check
res(pop_data) == res(leastcost_motor)
# True
# plot(pop_data)

## Has also clipped raster to match extent of 'leastcost_motor'
extent(pop_data) == extent(leastcost_motor)
# True
crs(pop_data)
# WGS84
crs(leastcost_motor)
# WGS84

## merge the pop_data and the leastcost_motor ???
leastcost_motor_pop <- mosaic(pop_data, leastcost_motor, fun = min, tolerance = 1)

leastcost_motor_pop
extent(leastcost_motor_pop) == extent(leastcost_motor)
# True!
crs(leastcost_motor_pop)
# WGS84

# Save raster
writeRaster(leastcost_motor_pop,
            "./WorldPop/pop_data",
            format = "GTiff", 
            overwrite=TRUE)



### 30 minutes

# Create data.frame from 'lcm_df'
leastcost_motor_pop_df <- as.data.frame(lcm_df, xy = TRUE)
View(leastcost_motor_pop_df)

# Subset data.frame by <30 minutes
thirty_min <- leastcost_motor_pop_df[leastcost_motor_pop_df$mins < 30, ]

# Remove 'layer' collumn to give just three rows (needed for 'rasterfromXYZ' function)
thirty_min$layer <- NULL

# Create raster
thirty_min_raster <- rasterFromXYZ(thirty_min)

# Check
# View(thirty_min_raster)
thirty_min_raster
# RasterLayer
# No CRS assigned...
plot(thirty_min_raster)

# Create polygon (takes ~5 mins)
thirty_min_polygon <- rasterToPolygons(thirty_min_raster, dissolve = TRUE)

# Check
thirty_min_polygon
# Is a SpatialPolygonsDataFrame (spdf)
# Has no CRS? Attempted to assign WGS84 but was then unable to use plot() function...
plot(thirty_min_polygon)
# A solid shape, whereas we need boundary vector

# Aggregate 'thirty_min_polygon' to give boundary 
thirty_min_polygoy_agg <- aggregate(thirty_min_polygon)
# Check
plot(thirty_min_polygoy_agg)

# Extract and sum population data from 'pop_data' by using 'thirty_min_polygoy_agg'
thirty_min_pop <- extract(pop_data, thirty_min_polygoy_agg, fun = sum, na.rm = TRUE)
thirty_min_pop
# 281609.2


### 60 minutes

# Create data.frame from 'lcm_df'
leastcost_motor_pop_df <- as.data.frame(lcm_df, xy = TRUE)
View(leastcost_motor_pop_df)

# Subset data.frame by <60 minutes
sixty_min <- leastcost_motor_pop_df[leastcost_motor_pop_df$mins < 60, ]

# Remove 'layer' collumn to give just three rows (needed for 'rasterfromXYZ' function)
sixty_min$layer <- NULL

View(sixty_min)

# Create raster
sixty_min_raster <- rasterFromXYZ(sixty_min)

# Check
# View(sixty_min_raster)
sixty_min_raster
# RasterLayer
# No CRS assigned...
plot(sixty_min_raster)

# Create polygon (takes ~10 mins)
sixty_min_polygon <- rasterToPolygons(sixty_min_raster, dissolve = TRUE)

# Check
sixty_min_polygon
# Is a SpatialPolygonsDataFrame (spdf)
# Has no CRS? Attempted to assign WGS84 but was then unable to use plot() function...
plot(sixty_min_polygon)
# A solid shape, whereas we need boundary vector

# Aggregate 'sixty_min_polygon' to give boundary 
sixty_min_polygoy_agg <- aggregate(sixty_min_polygon)
# Check
plot(sixty_min_polygoy_agg)

# Extract and sum population data from 'pop_data' by using 'sixty_min_polygoy_agg'
sixty_min_pop <- extract(pop_data, sixty_min_polygoy_agg, fun = sum, na.rm = TRUE)
sixty_min_pop
# 799607.3

```
### Working but generting polygon for time-boundaries takes a long time (30 min ~5 mins to generate, 60 min ~20 mins to generate...)


### Option_4: Download population rasters from WorldPop, extract from population rasters using reclasified time-boundary rasters (rather than polygons) (Working)
```{r reclassify_and_extract, include=FALSE}

## Reclassify 'leastcost_motor' and extract from 'pop_data'

# Create rasterlayer from 'lcm_df' (remove 'layer' column prior)
View(lcm_df)
lcm_df$layer = NULL
View(lcm_df)

lcm_raster <- rasterFromXYZ(lcm_df)
plot(lcm_raster)

# Create data.frame
rcl_matrix <- c(0, 30, 1, # 30 minutes
                30, 60, 2, # 1 hour
                60, 180, 3, # 3 hours
                180, 360, 4, # 6 hours 
                360, 720, 5, # 12 hours
                720, 100000, 6 # 12+ hours
)

rcl_matrix <- matrix(rcl_matrix, ncol=3, byrow=TRUE)
View(rcl_matrix)

# Reclassify 'lcm_raster' with 'rcl_matrix' 
lcm_pop_data_rcl <- reclassify(lcm_raster, rcl_matrix, include.lowest=TRUE)

# Check as data.frame
lcm_rcl_pop_data_df <- as.data.frame(lcm_pop_data_rcl, xy = TRUE)
View(lcm_rcl_pop_data_df)

# Zonal stuff
lcm_rcl_zone <- zonal(pop_data, lcm_pop_data_rcl, fun = sum)

# View
lcm_rcl_zone_df <- as.data.frame(lcm_rcl_zone, xy = TRUE)
View(lcm_rcl_zone_df)

# Rename columns
colnames(lcm_rcl_zone_df)
names(lcm_rcl_zone_df)[1] <- "Zone"
names(lcm_rcl_zone_df)[2] <- "Sum"
View(lcm_rcl_zone_df)

# Accumulate 
lcm_rcl_zone_df$Population <-
  
   c(lcm_rcl_zone_df$Sum[1],
     lcm_rcl_zone_df$Sum[2] + lcm_rcl_zone_df$Population[1],
     lcm_rcl_zone_df$Sum[3] + lcm_rcl_zone_df$Population[2],
     lcm_rcl_zone_df$Sum[4] + lcm_rcl_zone_df$Population[3],
     lcm_rcl_zone_df$Sum[5] + lcm_rcl_zone_df$Population[4],
     lcm_rcl_zone_df$Sum[6] + lcm_rcl_zone_df$Population[5])


View(lcm_rcl_zone_df)
print(lcm_rcl_zone_df)

```

